{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7295eb1b-5163-48b1-ade2-1dfa6fa2eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "218de0e2-1fc5-41d6-b098-57ff284ce6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antonplaksin\\AppData\\Local\\Temp\\ipykernel_13388\\2520818530.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = self.softmax(logits).data.numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 mean total reward: 26.6\n",
      "iteration: 1 mean total reward: 27.4\n",
      "iteration: 2 mean total reward: 24.8\n",
      "iteration: 3 mean total reward: 28.7\n",
      "iteration: 4 mean total reward: 30.45\n",
      "iteration: 5 mean total reward: 30.2\n",
      "iteration: 6 mean total reward: 41.4\n",
      "iteration: 7 mean total reward: 46.7\n",
      "iteration: 8 mean total reward: 54.6\n",
      "iteration: 9 mean total reward: 57.55\n",
      "iteration: 10 mean total reward: 44.4\n",
      "iteration: 11 mean total reward: 50.5\n",
      "iteration: 12 mean total reward: 51.9\n",
      "iteration: 13 mean total reward: 66.8\n",
      "iteration: 14 mean total reward: 51.25\n",
      "iteration: 15 mean total reward: 66.35\n",
      "iteration: 16 mean total reward: 64.85\n",
      "iteration: 17 mean total reward: 77.9\n",
      "iteration: 18 mean total reward: 70.65\n",
      "iteration: 19 mean total reward: 64.95\n",
      "iteration: 20 mean total reward: 61.05\n",
      "iteration: 21 mean total reward: 80.9\n",
      "iteration: 22 mean total reward: 74.35\n",
      "iteration: 23 mean total reward: 91.25\n",
      "iteration: 24 mean total reward: 100.1\n",
      "iteration: 25 mean total reward: 113.25\n",
      "iteration: 26 mean total reward: 120.5\n",
      "iteration: 27 mean total reward: 97.35\n",
      "iteration: 28 mean total reward: 129.05\n",
      "iteration: 29 mean total reward: 168.2\n",
      "iteration: 30 mean total reward: 165.8\n",
      "iteration: 31 mean total reward: 164.2\n",
      "iteration: 32 mean total reward: 166.5\n",
      "iteration: 33 mean total reward: 171.4\n",
      "iteration: 34 mean total reward: 223.5\n",
      "iteration: 35 mean total reward: 235.35\n",
      "iteration: 36 mean total reward: 235.95\n",
      "iteration: 37 mean total reward: 252.65\n",
      "iteration: 38 mean total reward: 290.9\n",
      "iteration: 39 mean total reward: 288.85\n",
      "iteration: 40 mean total reward: 293.0\n",
      "iteration: 41 mean total reward: 273.45\n",
      "iteration: 42 mean total reward: 366.2\n",
      "iteration: 43 mean total reward: 371.2\n",
      "iteration: 44 mean total reward: 331.7\n",
      "iteration: 45 mean total reward: 346.2\n",
      "iteration: 46 mean total reward: 388.25\n",
      "iteration: 47 mean total reward: 369.7\n",
      "iteration: 48 mean total reward: 394.55\n",
      "iteration: 49 mean total reward: 370.1\n",
      "iteration: 50 mean total reward: 341.5\n",
      "iteration: 51 mean total reward: 410.3\n",
      "iteration: 52 mean total reward: 362.9\n",
      "iteration: 53 mean total reward: 348.9\n",
      "iteration: 54 mean total reward: 376.5\n",
      "iteration: 55 mean total reward: 341.0\n",
      "iteration: 56 mean total reward: 364.5\n",
      "iteration: 57 mean total reward: 314.7\n",
      "iteration: 58 mean total reward: 381.35\n",
      "iteration: 59 mean total reward: 347.35\n",
      "iteration: 60 mean total reward: 399.45\n",
      "iteration: 61 mean total reward: 392.3\n",
      "iteration: 62 mean total reward: 386.1\n",
      "iteration: 63 mean total reward: 389.1\n",
      "iteration: 64 mean total reward: 413.8\n",
      "iteration: 65 mean total reward: 392.75\n",
      "iteration: 66 mean total reward: 391.95\n",
      "iteration: 67 mean total reward: 399.35\n",
      "iteration: 68 mean total reward: 432.35\n",
      "iteration: 69 mean total reward: 394.25\n",
      "iteration: 70 mean total reward: 420.4\n",
      "iteration: 71 mean total reward: 397.6\n",
      "iteration: 72 mean total reward: 386.35\n",
      "iteration: 73 mean total reward: 415.8\n",
      "iteration: 74 mean total reward: 396.75\n",
      "iteration: 75 mean total reward: 373.6\n",
      "iteration: 76 mean total reward: 399.65\n",
      "iteration: 77 mean total reward: 383.15\n",
      "iteration: 78 mean total reward: 419.55\n",
      "iteration: 79 mean total reward: 446.2\n",
      "iteration: 80 mean total reward: 414.75\n",
      "iteration: 81 mean total reward: 433.3\n",
      "iteration: 82 mean total reward: 367.8\n",
      "iteration: 83 mean total reward: 434.85\n",
      "iteration: 84 mean total reward: 403.9\n",
      "iteration: 85 mean total reward: 405.25\n",
      "iteration: 86 mean total reward: 340.2\n",
      "iteration: 87 mean total reward: 457.4\n",
      "iteration: 88 mean total reward: 376.15\n",
      "iteration: 89 mean total reward: 451.2\n",
      "iteration: 90 mean total reward: 421.95\n",
      "iteration: 91 mean total reward: 417.65\n",
      "iteration: 92 mean total reward: 447.4\n",
      "iteration: 93 mean total reward: 416.5\n",
      "iteration: 94 mean total reward: 436.05\n",
      "iteration: 95 mean total reward: 421.95\n",
      "iteration: 96 mean total reward: 425.2\n",
      "iteration: 97 mean total reward: 409.45\n",
      "iteration: 98 mean total reward: 425.55\n",
      "iteration: 99 mean total reward: 433.3\n",
      "total reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_dim = 4\n",
    "action_n = 2\n",
    "\n",
    "\n",
    "class CEM(nn.Module):\n",
    "    def __init__(self, state_dim, action_n):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_n = action_n\n",
    "        self.network = nn.Sequential(nn.Linear(self.state_dim, 128), \n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(128, self.action_n)\n",
    "                                    )\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.optimazer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, _input):\n",
    "        return self.network(_input)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits = self.forward(state)\n",
    "        probs = self.softmax(logits).data.numpy()\n",
    "        action = np.random.choice(self.action_n, p=probs)\n",
    "        return action\n",
    "\n",
    "    def fit(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            for state, action in zip(trajectory['states'], trajectory['actions']):\n",
    "                elite_states.append(state)\n",
    "                elite_actions.append(action)\n",
    "        elite_states = torch.FloatTensor(elite_states)\n",
    "        elite_actions = torch.LongTensor(elite_actions)\n",
    "        pred_actions = self.forward(elite_states)\n",
    "\n",
    "        loss = self.loss(pred_actions, elite_actions)\n",
    "        loss.backward()\n",
    "        self.optimazer.step()\n",
    "        self.optimazer.zero_grad()\n",
    "        \n",
    "\n",
    "def get_trajectory(env, agent, max_len=1000, visualize=False):\n",
    "    trajectory = {'states': [], 'actions': [], 'rewards': []}\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trajectory['states'].append(state)\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        trajectory['actions'].append(action)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        trajectory['rewards'].append(reward)\n",
    "\n",
    "        if visualize:\n",
    "            time.sleep(0.5)\n",
    "            env.render()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "\n",
    "agent = CEM(state_dim, action_n)\n",
    "q_param = 0.8\n",
    "iteration_n = 100\n",
    "trajectory_n = 20\n",
    "trajectory_len = 500\n",
    "\n",
    "for iteration in range(iteration_n):\n",
    "\n",
    "    #policy evaluation\n",
    "    trajectories = [get_trajectory(env, agent) for _ in range(trajectory_n)]\n",
    "    total_rewards = [np.sum(trajectory['rewards']) for trajectory in trajectories]\n",
    "    print('iteration:', iteration, 'mean total reward:', np.mean(total_rewards))\n",
    "\n",
    "    #policy improvement\n",
    "    quantile = np.quantile(total_rewards, q_param)\n",
    "    elite_trajectories = []\n",
    "    for trajectory in trajectories:\n",
    "        total_reward = np.sum(trajectory['rewards'])\n",
    "        if total_reward > quantile:\n",
    "            elite_trajectories.append(trajectory)\n",
    "    \n",
    "    if len(elite_trajectories) > 0:\n",
    "        agent.fit(elite_trajectories)\n",
    "\n",
    "trajectory = get_trajectory(env, agent, max_len=500, visualize=False)\n",
    "print('total reward:', sum(trajectory['rewards']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d836f-b2c3-41ef-a063-000c1df23b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
